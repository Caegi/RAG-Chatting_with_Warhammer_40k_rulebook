{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w6W-_gbjqBB-",
        "o2LuomKvFC6f",
        "IQJyWLRirKUG",
        "oATSRez1GQdC",
        "t0hWBAt-Df7N"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoG7Z6PRakCq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install chromadb\n",
        "!pip install -U bitsandbytes\n",
        "!pip install llama-cpp-python\n",
        "!pip install rank_bm25 nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "\n",
        "Building a simple RAG for question answering based on a light quantized LLama3.2 1B model. The goal is to answer accurately questions concerning Warhammer 40K rules.\n",
        "\n",
        "## Overview\n",
        "\n",
        "First we will import Llama 3.2 and try some templatting and chat with the model.\n",
        "\n",
        "Secondly, we will experiment with [ChromaDB](https://docs.trychroma.com/getting-started) and build a first RAG.\n",
        "\n",
        "Finally, we will be using the headers and BM25 to try and improve the retriever.\n",
        "\n",
        "# Imports"
      ],
      "metadata": {
        "id": "Woou6eQ-ti7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import json\n",
        "import uuid\n",
        "\n",
        "from llama_cpp import Llama\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from jinja2 import Template\n",
        "\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import spacy"
      ],
      "metadata": {
        "id": "OvJmaR5Z7MTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Model\n",
        "\n",
        "## CPU implementation"
      ],
      "metadata": {
        "id": "wXtY3uiahEhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama.from_pretrained(\n",
        "      repo_id=\"bartowski/Llama-3.2-1B-Instruct-GGUF\",\n",
        "      filename=\"*Q8_0.gguf\",\n",
        "      verbose=False,\n",
        "      n_ctx=25000,\n",
        ")"
      ],
      "metadata": {
        "id": "3v2WV5rocFoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llama_cpp_generate(prompt):\n",
        "  output = llm(prompt, max_tokens=300)\n",
        "  return output[\"choices\"][0][\"text\"]\n",
        "\n",
        "llama_cpp_generate(\"What is the capital of France ?\")"
      ],
      "metadata": {
        "id": "YszQtcyscCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Implementation"
      ],
      "metadata": {
        "id": "9XLfeC37h8GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")"
      ],
      "metadata": {
        "id": "Ht6D8FnjRN50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_complete(prompt, max_tokens=300):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    max_length = len(inputs[\"input_ids\"][0]) + max_tokens\n",
        "    outputs = model.generate(**inputs, max_length=max_length)\n",
        "    generated_tokens = outputs[0][len(inputs[\"input_ids\"][0]):]  # Exclude the input prompt tokens\n",
        "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Fm_0lz7f1adK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial LLM experiments\n",
        "\n",
        "### Prompt Template"
      ],
      "metadata": {
        "id": "n_PcFlbricLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = Template(\n",
        "    \"\"\"\n",
        "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    {{role}}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    {{input}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "Vaxz8clU67-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Playing with the LLM"
      ],
      "metadata": {
        "id": "NGY-b6Pgi2LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {'role':'you are a depressed miserable sad clown' ,'input': 'tell me a joke !'}\n",
        "input_2 = {'role':'you are a clown for children' ,'input': 'tell me a joke !'}"
      ],
      "metadata": {
        "id": "TBxuNhng79hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLama cpp"
      ],
      "metadata": {
        "id": "XTuos3o9vh6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.render(role=input[\"role\"], input=input[\"input\"])\n",
        "llama_cpp_generate(prompt)"
      ],
      "metadata": {
        "id": "Dj8Zi-45Rf2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2 = prompt_template.render(role=input_2[\"role\"], input=input_2[\"input\"])\n",
        "llama_cpp_generate(prompt_2)"
      ],
      "metadata": {
        "id": "rcO691MVX4Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsloth Llama 3.2"
      ],
      "metadata": {
        "id": "Zx1psGkRvkY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_complete(prompt)"
      ],
      "metadata": {
        "id": "xy4FFdvXvzvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_complete(prompt_2)"
      ],
      "metadata": {
        "id": "g0D_mLmRv0V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you think ?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKQafvd_-GRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It behaves quite differently so the role is important"
      ],
      "metadata": {
        "id": "7yhW3xghLMoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever"
      ],
      "metadata": {
        "id": "nRupn_MwAR6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb.utils.embedding_functions as embedding_functions"
      ],
      "metadata": {
        "id": "m43EsLN3CI3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "client = chromadb.Client()\n",
        "stf_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "vR5FLZ60_XKz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = client.create_collection(name=\"warhammer_40k\",\n",
        "                                      metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                      embedding_function=stf_function)"
      ],
      "metadata": {
        "id": "MG_pDO0oBi8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/my_first_rag_data.zip"
      ],
      "metadata": {
        "id": "wpyVIIOzenCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('chunks.json', 'r') as f:\n",
        "    chunks = json.load(f)"
      ],
      "metadata": {
        "id": "vqJldKIUCip5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the documents to the collection\n",
        "collection.add(\n",
        "    documents=[doc[\"page_content\"] for doc in chunks],\n",
        "    ids=[str(i) for i in range(len(chunks))],\n",
        "    metadatas=[doc[\"metadata\"] for doc in chunks]\n",
        ")"
      ],
      "metadata": {
        "id": "cyjNUrkOmgZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is a visible unit ?\"\n",
        "\n",
        "# Performing a query\n",
        "collection.query(\n",
        "    query_texts=[question],\n",
        "    n_results=4\n",
        ")"
      ],
      "metadata": {
        "id": "AdghQbnpCt36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a retrieval function wrapping the ChromaDB query and returning an adapted format."
      ],
      "metadata": {
        "id": "loMw0Bv0EunN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns list[dict[str:str]], each dictionnary represents a chunk\n",
        "def retrieve(question, n_results=5):\n",
        "    res_query = collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    chunk_list = [{\"header\":res_query[\"metadatas\"][0][i],\n",
        "                   \"text\":res_query[\"documents\"][0][i]} for i in range(n_results)]\n",
        "\n",
        "    return chunk_list"
      ],
      "metadata": {
        "id": "71Lu8EiWFAr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rag template\n",
        "\n",
        "Create a RAG template in Jinja"
      ],
      "metadata": {
        "id": "KtRvx_u6jpZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do a loop inside\n",
        "rag_template = Template(\n",
        "  \"\"\"\n",
        "  <|begin_of_text|>\n",
        "\n",
        "  <|start_header_id|>system<|end_header_id|>\n",
        "  {{role}}\n",
        "  <|eot_id|>\n",
        "\n",
        "  <|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "  Use the context and only the context to answer the following question:\n",
        "\n",
        "  Question:\n",
        "\n",
        "  {{question}}\n",
        "\n",
        "  Context:\n",
        "  {% for chunk in chunks %}\n",
        "  {% for key, value in chunk.header.items() %}{{ key }}: {{ value  }} {% endfor %}\n",
        "  text: {{chunk.text}}\n",
        "  {% endfor %}\n",
        "  <|eot_id|>\n",
        "\n",
        "  <|start_header_id|>assistant<|end_header_id|>\n",
        "  \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "XsOWaJxGjuzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_template.render(**{\n",
        "    'role': 'you are an experienced wargame player',\n",
        "    'question': \"What is a visible unit ?\",\n",
        "    'chunks':[{'header': {'header1':'toto'},'text':'ctx1'},{'header': {'header1':'tato', 'header2':'tato'},'text':'ctx2'},{'header': {'header1':'tato'}, 'text':'ctx3'}]\n",
        "}))"
      ],
      "metadata": {
        "id": "BnktYC5sj5xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a template that will create the prompt using the question and chunks."
      ],
      "metadata": {
        "id": "PzP5AKsRkCA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a prompt\n",
        "def prompt_generation(question, chunks):\n",
        "    return rag_template.render(**{\n",
        "      'role': 'you are an experienced wargame player',\n",
        "      'question': question,\n",
        "      'chunks': chunks\n",
        "    })"
      ],
      "metadata": {
        "id": "TKe-DnAykBYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_generation(question, retrieve(question)))"
      ],
      "metadata": {
        "id": "jLUqmJN2Fffc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Rag\n",
        "\n",
        "Create functions to perform the full RAG pipeline, you may create a function for the CPU and another one for the GPU."
      ],
      "metadata": {
        "id": "PvBFifq7F1h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_0 = \"What is a visible unit ?\"\n",
        "question_1 = 'What are the limitations associated to the advance mouvement rule ?'\n",
        "question_2 = 'Is there a stratagem that can be used to reroll a failed dice role?'\n",
        "question_3 = 'Explain the Comand Re-roll stratagem'"
      ],
      "metadata": {
        "id": "4yjRG9hqkwcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_rag_cpu(question, n_results=4):\n",
        "  prompt = prompt_generation(question,retrieve(question))\n",
        "  answer = llama_cpp_generate(prompt)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "DuAGBaenFsZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_cpu(question_0)"
      ],
      "metadata": {
        "id": "S2IJdT0Rg_4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_cpu(question_1)"
      ],
      "metadata": {
        "id": "y8dHrOJkhIS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_cpu(question_2)"
      ],
      "metadata": {
        "id": "iUcYpzFDhKPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_cpu(question_3)"
      ],
      "metadata": {
        "id": "p6iDKxHWhL0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "vKqosCoCE40k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def full_rag_gpu(question, n_results=4):\n",
        "    prompt = prompt_generation(question,retrieve(question))\n",
        "    answer = llm_complete(prompt)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "8TP1wD7SUTJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_gpu(question_0)"
      ],
      "metadata": {
        "id": "wh4ksSgBE92a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_gpu(question_1)"
      ],
      "metadata": {
        "id": "WTWV_JvLFBNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_gpu(question_2)"
      ],
      "metadata": {
        "id": "ou8n0YJVFEIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_gpu(question_3)"
      ],
      "metadata": {
        "id": "hhEv17cRFFR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a reranker\n",
        "The results Might not be satisfactory for some questions.\n",
        "\n",
        "In order to better use the header extraction, we will rerank the chunks using BM25 over the headers."
      ],
      "metadata": {
        "id": "flMmij9HWBTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenization"
      ],
      "metadata": {
        "id": "w6W-_gbjqBB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_stopwords_set = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "_nZXd0BnNrw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_without_stopwords(s, stopwords_set=set()):\n",
        "  return [token.text for token in nlp(s) if token.text not in stopwords_set]\n",
        "\n",
        "# Testing tokenization\n",
        "test_tokenized = tokenize_without_stopwords(question_2, eng_stopwords_set)\n",
        "test_tokenized"
      ],
      "metadata": {
        "id": "jrHIECcMnW2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stemming"
      ],
      "metadata": {
        "id": "o2LuomKvFC6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(language=\"english\")\n",
        "\n",
        "# returns list[str]\n",
        "def stem(l_tokens):\n",
        "  return [stemmer.stem(token) for token in l_tokens]\n",
        "\n",
        "test_stemmed = stem(test_tokenized)\n",
        "test_stemmed"
      ],
      "metadata": {
        "id": "m3Ht8gSuNvVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing"
      ],
      "metadata": {
        "id": "IQJyWLRirKUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_preprocessing(metadata):\n",
        "    # medata: list[str]\n",
        "    tokenized = [tokenize_without_stopwords(s) for s in metadata]\n",
        "    preprocessed_headers = [stem(l_tokens) for l_tokens in tokenized]\n",
        "    return preprocessed_headers # list[list[str]]\n",
        "\n",
        "def query_preprocessing(query):\n",
        "    # query: str\n",
        "    tokenized = tokenize_without_stopwords(query)\n",
        "    preprocessed_queries = stem(tokenized)\n",
        "    return preprocessed_queries # list[str]"
      ],
      "metadata": {
        "id": "9TW-taRDMzsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BM25 engine"
      ],
      "metadata": {
        "id": "oATSRez1GQdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_chunks(question, chunks, n_results=5):\n",
        "    headers = ['#'.join(chunk[\"header\"].values()) for chunk in chunks] # list[str]\n",
        "    preprocessed_headers = metadata_preprocessing(headers)\n",
        "    preprocessed_query = query_preprocessing(question)\n",
        "\n",
        "    bm25_model = BM25Okapi(preprocessed_headers)\n",
        "\n",
        "    scores = bm25_model.get_scores(preprocessed_query)\n",
        "    chunks_score_list = list(zip(chunks, scores)) # list[ tuple( dict[str:str], float ) ]\n",
        "    chunks_score_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_chunks = chunks_score_list[:n_results]\n",
        "\n",
        "    return best_chunks # list[ tuple( dict[str:str], float ) ], returns best chunks and their score"
      ],
      "metadata": {
        "id": "KS90kln0aZnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tests"
      ],
      "metadata": {
        "id": "t0hWBAt-Df7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_chunks = retrieve(question_0, n_results=100) # list[dict[str:str]]\n",
        "reranked_chunks = rerank_chunks(question_0, retrieved_chunks, n_results=10) # list[ tuple( dict[str:str],float ) ]\n",
        "for rr_chk in reranked_chunks:\n",
        "    print('score: {}'.format(rr_chk[1]))\n",
        "    print('header: {}'.format('#'.join(rr_chk[0]['header'].values())))\n",
        "    print('text: {}'.format(rr_chk[0]['text']))\n",
        "    print('-'*100)"
      ],
      "metadata": {
        "id": "243wM3cs9cKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_chunks = retrieve(question_1, n_results=100) # list[dict[str:str]]\n",
        "reranked_chunks = rerank_chunks(question_1, retrieved_chunks, n_results=10) # list[ tuple( dict[str:str],float ) ]\n",
        "for rr_chk in reranked_chunks:\n",
        "    print('score: {}'.format(rr_chk[1]))\n",
        "    print('header: {}'.format('#'.join(rr_chk[0]['header'].values())))\n",
        "    print('text: {}'.format(rr_chk[0]['text']))\n",
        "    print('-'*100)"
      ],
      "metadata": {
        "id": "WjcW-xWuECsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_chunks = retrieve(question_2, n_results=100)\n",
        "reranked_chunks = rerank_chunks(question_2, retrieved_chunks, n_results=10)\n",
        "for rr_chk in reranked_chunks:\n",
        "    print('score: {}'.format(rr_chk[1]))\n",
        "    print('header: {}'.format('#'.join(rr_chk[0]['header'].values())))\n",
        "    print('text: {}'.format(rr_chk[0]['text']))\n",
        "    print('-'*100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5_NX0XBbaZjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_3 = 'Explain the Comand Re-roll stratagem'\n",
        "retrieved_chunks = retrieve(question_3, n_results=100)\n",
        "reranked_chunks = rerank_chunks(question_3, retrieved_chunks, n_results=10)\n",
        "for rr_chk in reranked_chunks:\n",
        "    print('score: {}'.format(rr_chk[1]))\n",
        "    print('header: {}'.format('#'.join(rr_chk[0]['header'].values())))\n",
        "    print('text: {}'.format(rr_chk[0]['text']))\n",
        "    print('-'*100)"
      ],
      "metadata": {
        "id": "dT-rCxncaZgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with reranker"
      ],
      "metadata": {
        "id": "Ho5kRgfVZnn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = [] # list[dict[str:str]]\n",
        "\n",
        "def full_rag_reranker_cpu(question, n_results=5):\n",
        "  retrieved_chunks = retrieve(question, n_results=100) # list[dict[str:str]]\n",
        "  reranked_chunks_with_scores = rerank_chunks(question, retrieved_chunks, n_results=5) # list[ tuple( dict[str:str],float ) ]\n",
        "  reranked_chunks = [tuple_chk_score[0] for tuple_chk_score in reranked_chunks_with_scores]\n",
        "  prompt = prompt_generation(question,reranked_chunks)\n",
        "  answer = llama_cpp_generate(prompt)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "KLjtbwIAZnHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How can I win the game?\""
      ],
      "metadata": {
        "id": "Fj24vKheqKTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with default 5 chunks as context"
      ],
      "metadata": {
        "id": "VP9J2QWcFNMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_cpu(question)"
      ],
      "metadata": {
        "id": "DD1RCnFEqKQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_reranker_cpu(question)"
      ],
      "metadata": {
        "id": "3no27yPqqKMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "YEaLNdkaFlNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def full_rag_reranker_gpu(question, n_results=5):\n",
        "  retrieved_chunks = retrieve(question, n_results=100) # list[dict[str:str]]\n",
        "  reranked_chunks_with_scores = rerank_chunks(question, retrieved_chunks, n_results=5) # list[ tuple( dict[str:str],float ) ]\n",
        "  reranked_chunks = [tuple_chk_score[0] for tuple_chk_score in reranked_chunks_with_scores]\n",
        "  prompt = prompt_generation(question,reranked_chunks)\n",
        "  answer = llm_complete(prompt)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "Cptw-rsCHLlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_gpu(question)"
      ],
      "metadata": {
        "id": "kq6reBQ6HE9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_rag_reranker_gpu(question)"
      ],
      "metadata": {
        "id": "Y9S1cS7uHHxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is better using full_rag_reranker: it is more detailed and relevant"
      ],
      "metadata": {
        "id": "dmc7176LHoBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatting with the rulebook\n",
        "\n",
        "At this point, we created a single question/answer turn RAG. It can be usefull for some applications to allow conversations with documents."
      ],
      "metadata": {
        "id": "M7s4lRyefmD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = [] # list[dict[str:str]], contains the chat history"
      ],
      "metadata": {
        "id": "RndrLFMEhe4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do a loop inside\n",
        "chat_rag_template = Template(\n",
        "  \"\"\"\n",
        "  <|begin_of_text|>\n",
        "\n",
        "  <|start_header_id|>system<|end_header_id|>\n",
        "  {{role}}\n",
        "  <|eot_id|>\n",
        "\n",
        "  {% for chat in history %}\n",
        "  <|start_header_id|>user<|end_header_id|>\n",
        "  {{chat.h_question}}\n",
        "  <|eot_id|>\n",
        "\n",
        "  <|start_header_id|>assistant<|end_header_id|>\n",
        "  {{chat.h_answer}}\n",
        "  <|eot_id|>\n",
        "  {% endfor %}\n",
        "\n",
        "  <|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "  Use the context and only the context to answer the following question:\n",
        "\n",
        "  Question:\n",
        "\n",
        "  {{question}}\n",
        "\n",
        "  Context:\n",
        "  {% for chunk in chunks %}\n",
        "  {% for key, value in chunk.header.items() %}{{ key }}: {{ value  }} {% endfor %}\n",
        "  text: {{chunk.text}}\n",
        "  {% endfor %}\n",
        "  <|eot_id|>\n",
        "\n",
        "  <|start_header_id|>assistant<|end_header_id|>\n",
        "  \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "xqnPPl4ofDaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_prompt_generation(question, chunks, history):\n",
        "    return chat_rag_template.render(**{\n",
        "    'role': 'you are an experienced wargame player',\n",
        "    'question': question, # str\n",
        "    'chunks': chunks, # list[dict[str:str]]\n",
        "    'history': history # list[dict[str:str]]\n",
        "    })"
      ],
      "metadata": {
        "id": "PjW_6CWhe4Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_rag_gpu(question, n_results=5, clear_history=False):\n",
        "  if clear_history: history.clear()\n",
        "\n",
        "  retrieved_chunks = retrieve(question, n_results=100) # list[dict[str:str]]\n",
        "  reranked_chunks_with_scores = rerank_chunks(question, retrieved_chunks, n_results=5) # list[ tuple( dict[str:str],float ) ]\n",
        "  reranked_chunks = [tuple_chk_score[0] for tuple_chk_score in reranked_chunks_with_scores]\n",
        "  prompt = chat_prompt_generation(question, reranked_chunks, history)\n",
        "  answer = llm_complete(prompt)\n",
        "\n",
        "  # Only add question and answer to limit going over token limit in prompt\n",
        "  current_chat = {\"h_question\":question, \"h_answer\":answer}\n",
        "  history.append(current_chat)\n",
        "\n",
        "  return answer"
      ],
      "metadata": {
        "id": "f5kEMFWfOINI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_rag_gpu(question, clear_history=True)"
      ],
      "metadata": {
        "id": "cBFo-lu8JX-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_rag_gpu(\"What are the victory conditions ?\")"
      ],
      "metadata": {
        "id": "SHBBzHuAN5c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_rag_gpu(\"What objective markers are there ?\")"
      ],
      "metadata": {
        "id": "d_4JBtf_mEFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}